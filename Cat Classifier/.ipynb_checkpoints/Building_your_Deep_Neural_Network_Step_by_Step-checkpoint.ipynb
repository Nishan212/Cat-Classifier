{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network: Step by Step -> Building a Cat classifier\n",
    "\n",
    "- A Deep neural network with as many layers as we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- dnn_utils provides some necessary functions for this notebook.\n",
    "- testCases provides some test cases to assess the correctness of your functions\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v4a import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Initialization\n",
    "### L-layer Neural Network\n",
    "\n",
    "-The is initialization for a deeper L-layer neural network. $n^{[l]}$ is the number of units in layer $l$. Thus for example if the size of our input $X$ is $(12288, 209)$ (with $m=209$ examples) then:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "<tr>\n",
    "    <td>  </td> \n",
    "    <td> Shape of W </td> \n",
    "    <td> Shape of b  </td> \n",
    "    <td> Activation </td>\n",
    "    <td> Shape of Activation </td> \n",
    "<tr>\n",
    "\n",
    "<tr>\n",
    "    <td> Layer 1 </td> \n",
    "    <td> $(n^{[1]},12288)$ </td> \n",
    "    <td> $(n^{[1]},1)$ </td> \n",
    "    <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "    <td> $(n^{[1]},209)$ </td> \n",
    "<tr>\n",
    "\n",
    "<tr>\n",
    "    <td> Layer 2 </td> \n",
    "    <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "    <td> $(n^{[2]},1)$ </td> \n",
    "    <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "    <td> $(n^{[2]}, 209)$ </td> \n",
    "<tr>\n",
    "\n",
    "   <tr>\n",
    "    <td> $\\vdots$ </td> \n",
    "    <td> $\\vdots$  </td> \n",
    "    <td> $\\vdots$  </td> \n",
    "    <td> $\\vdots$</td> \n",
    "    <td> $\\vdots$  </td> \n",
    "<tr>\n",
    "    \n",
    "<tr>\n",
    "    <td> Layer L-1 </td> \n",
    "    <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "    <td> $(n^{[L-1]}, 1)$  </td> \n",
    "    <td> $Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "    <td> $(n^{[L-1]}, 209)$ </td> \n",
    "<tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "    <td> Layer L </td> \n",
    "    <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "    <td> $(n^{[L]}, 1)$ </td>\n",
    "    <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "    <td> $(n^{[L]}, 209)$  </td> \n",
    "<tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model's structure is *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e., it has $L-1$ layers using a ReLU activation function followed by an output layer with a sigmoid activation function.\n",
    "- Random initialization for the weight matrices -> `np.random.randn(shape) * 0.01`.\n",
    "- Zeros initialization for the biases -> `np.zeros(shape)`.\n",
    "- We will store $n^{[l]}$, the number of units in different layers, in a variable `layer_dims`.\n",
    "- Implementation for $L=1$ (one layer neural network).\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Forward propagation module\n",
    "\n",
    "###  3.1 - Linear Forward \n",
    "Three functions needed in the module:\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (whole model)\n",
    "\n",
    "The linear forward module (vectorized over all the examples) computes the following equations:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "where $A^{[0]} = X$. \n",
    "\n",
    "The mathematical representation of this unit is $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Linear-Activation Forward\n",
    "\n",
    "In this notebook, we will use two activation functions:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
    "A, activation_cache = sigmoid(Z)\n",
    "\n",
    "- **ReLU**: The mathematical formula for ReLu is $A = RELU(Z) = max(0, Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more convenience, the two functions (Linear and Activation) are grouped into one function (LINEAR->ACTIVATION). Hence, the function does the LINEAR forward step followed by an ACTIVATION forward step.\n",
    "\n",
    "Forward propagation of the *LINEAR->ACTIVATION* layer. Mathematical relation is: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation \"g\" can be sigmoid() or relu()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 L-Layer Model \n",
    "\n",
    "- For even more convenience when implementing the $L$-layer Neural Net is implemented using a function that replicates the previous one (`linear_activation_forward` with RELU) $L-1$ times, then follows that with one `linear_activation_forward` with SIGMOID.\n",
    "\n",
    "- In the code below, the variable `AL` denotes $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (This is sometimes also called `Yhat`, i.e., this is $\\hat{Y}$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the full forward propagation that takes the input X and outputs a row vector $A^{[L]}$ containing your predictions is implemented. It also records all intermediate values in \"caches\". Using $A^{[L]}$, you can compute the cost of your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost function (cross-entropy)\n",
    "\n",
    "- Cost computation to check if the model is actually learning.\n",
    "- Computing the cross-entropy cost $J$, using the following formula: $$ J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost =  -(np.dot(np.log(AL), Y.T) + np.dot(np.log(1 - AL), (1 - Y.T))) / m\n",
    "    # ^^ = -np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure the cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward propagation module\n",
    "\n",
    "Just like with forward propagation, Implementing helper functions for backpropagation. Back propagation is used to calculate the gradient of the loss function with respect to the parameters. \n",
    "\n",
    "The chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, use the previous chain rule and do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step multiply the current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, use the previous chain rule and do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "(This is why we talk about **backpropagation**.)\n",
    "\n",
    "Now, similar to forward propagation, building the backward propagation in three steps:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Linear backward\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose we have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. We want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas used:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    "Creating a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**. \n",
    "\n",
    "To help with `linear_activation_backward`, two backward functions are needed:\n",
    "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, we should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward \n",
    "\n",
    "Implementing the backward function for the whole network. \n",
    "In the back propagation module, we use those cached variables during forward propagation to compute the gradients. Therefore, in the `L_model_backward` function, we will iterate through all the hidden layers backward, starting from layer $L$. On each step, we will use the cached values for layer $l$ to backpropagate through layer $l$.\n",
    "\n",
    "**Initializing backpropagation**:\n",
    "To backpropagate through this network, the the output is, \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. The code thus needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "\n",
    "We can then use this post-activation gradient `dAL` to keep going backward. we can now feed in `dAL` into the LINEAR->SIGMOID backward function implemented (which will use the cached values stored by the L_model_forward function). After that, we will have to use a `for` loop to iterate through all the other layers using the LINEAR->RELU backward function. We store each dA, dW, and db in the grads dictionary using this formula : \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "For example, for $l=3$ this would store $dW^{[l]}$ in `grads[\"dW3\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing backpropagation for the *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients.\n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Update Parameters\n",
    "\n",
    "In this section we will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, we will store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 -  Deep Neural Network for Image Classification: Application\n",
    "- Applying the deep neural network to supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dnn_app_utils provides some functions to load the data\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Dataset\n",
    "\n",
    "We will use the \"Cat vs non-Cat\" dataset.\n",
    "\n",
    "Dataset (\"data.h5\") contains:\n",
    "    - a training set of m_train images labelled as cat (1) or non-cat (0)\n",
    "    - a test set of m_test images labelled as cat and non-cat\n",
    "    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB).\n",
    "\n",
    "Load the data by running the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will show an image in the dataset. Feel free to change the index and re-run the cell multiple times to see other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1. It's a cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 25\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0. It's a non-cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 10\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Exploring the dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping and standardizing the images before feeding them to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten / 255.\n",
    "test_x = test_x_flatten / 255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$12,288$ equals $64 \\times 64 \\times 3$ which is the size of one reshaped image vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Architecture of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with the dataset, it is time to build a deep neural network to distinguish cat images from non-cat images.\n",
    "\n",
    "We will build an L-layer deep neural network\n",
    "\n",
    "### 3.1 - L-layer deep neural network\n",
    "\n",
    "- The input is a (64,64,3) image which is flattened to a vector of size (12288,1).\n",
    "- The corresponding vector: $[x_0,x_1,...,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ and then we add the intercept $b^{[1]}$. The result is the linear unit.\n",
    "- Next, we take the relu of the linear unit. This process could be repeated several times for each $(W^{[l]}, b^{[l]})$ depending on the model architecture.\n",
    "- Finally, we take the sigmoid of the final linear unit. If it is greater than 0.5, we classify it to be a cat.\n",
    "\n",
    "### 3.3 - General methodology\n",
    "\n",
    "We will follow this Deep Learning methodology to build the model:\n",
    "    1. Initialize parameters / Define hyperparameters\n",
    "    2. Loop for num_iterations:\n",
    "        a. Forward propagation\n",
    "        b. Compute cost function\n",
    "        c. Backward propagation\n",
    "        d. Update parameters (using parameters, and grads from backprop) \n",
    "    4. Use trained parameters to predict labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - L-layer Neural Network\n",
    "We use the helper functions we have implemented previously to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. The functions needed are:\n",
    "```python\n",
    "def initialize_parameters_deep(layers_dims):\n",
    "    ...\n",
    "    return parameters \n",
    "def L_model_forward(X, parameters):\n",
    "    ...\n",
    "    return AL, caches\n",
    "def compute_cost(AL, Y):\n",
    "    ...\n",
    "    return cost\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    ...\n",
    "    return grads\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the model as a 4-layer neural network. \n",
    "\n",
    "Run the cell below to train the model. The cost should decrease on every iteration. It may take up to 5 minutes to run 2500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Predict the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  11 -  Results Analysis\n",
    "\n",
    "Let's take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    \"\"\"\n",
    "    Plots images where predictions and truth were different.\n",
    "    X -- dataset\n",
    "    y -- true labels\n",
    "    p -- predictions\n",
    "    \"\"\"\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
